{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8cB9caodKq4",
        "outputId": "a795aeaf-3990-4fce-a370-90505465bda7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qwikidata\n",
            "  Downloading qwikidata-0.4.2.tar.gz (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting mypy-extensions (from qwikidata)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: qwikidata\n",
            "  Building wheel for qwikidata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qwikidata: filename=qwikidata-0.4.2-py3-none-any.whl size=24867 sha256=b97779428e0130a1038b77211fff95b9b6b04548c24a51067abee49920085a2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/a2/85/3ca91fc8f95fa5be840fce552ac382bbcddaea6d2e31212ae5\n",
            "Successfully built qwikidata\n",
            "Installing collected packages: mypy-extensions, qwikidata\n",
            "Successfully installed mypy-extensions-1.0.0 qwikidata-0.4.2\n"
          ]
        }
      ],
      "source": [
        "pip install qwikidata pandas requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFWVGUFmdLuv",
        "outputId": "65ab7b30-2a70-4347-e379-794b35796653"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.11.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.11.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=f3dd8b85fa038671a0137cda529a1e30b22cd6ae58b993f5a836928cc4f62ecc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.6\n",
            "    Uninstalling httpcore-1.0.6:\n",
            "      Successfully uninstalled httpcore-1.0.6\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.2\n",
            "    Uninstalling httpx-0.27.2:\n",
            "      Successfully uninstalled httpx-0.27.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.1.143 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.54.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from qwikidata.entity import WikidataItem\n",
        "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "\n",
        "# SPARQL endpoint for Wikidata Query Service\n",
        "WIKIDATA_SPARQL_URL = \"https://query.wikidata.org/sparql\"\n",
        "translator = Translator()  # Initialize the Google Translate API\n",
        "\n",
        "# List of possible occupations (you can expand this list)\n",
        "OCCUPATION_KEYWORDS = [\"physicist\", \"chemist\", \"mathematician\", \"biologist\", \"engineer\", \"astronomer\", \"scientist\"]\n",
        "\n",
        "# Function to execute a SPARQL query\n",
        "def execute_sparql_query(query):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"WikiDataQueryBot/0.1 (test@example.org)\"\n",
        "    }\n",
        "    response = requests.get(WIKIDATA_SPARQL_URL, params={'query': query, 'format': 'json'}, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "# Function to extract scientist data with added queries for awards and education\n",
        "def get_scientist_data(limit=10):\n",
        "    query = f\"\"\"\n",
        "    SELECT ?scientist ?scientistLabel ?birthDate ?deathDate ?birthPlaceLabel\n",
        "           (GROUP_CONCAT(DISTINCT ?awardLabel; SEPARATOR=\"; \") AS ?awards)\n",
        "           (GROUP_CONCAT(DISTINCT ?educationInstitutionLabel; SEPARATOR=\"; \") AS ?educationInstitutions)\n",
        "    WHERE {{\n",
        "      ?scientist wdt:P31 wd:Q5 ;  # Entity must be a human (Q5)\n",
        "                wdt:P106 wd:Q901 ;  # Must have the occupation of scientist (Q901)\n",
        "                wdt:P569 ?birthDate .  # Birth date (P569)\n",
        "      OPTIONAL {{ ?scientist wdt:P570 ?deathDate. }}  # Date of death (P570)\n",
        "      OPTIONAL {{ ?scientist wdt:P19 ?birthPlace. }}\n",
        "\n",
        "      # Fetch awards\n",
        "      OPTIONAL {{\n",
        "        ?scientist wdt:P166 ?award.\n",
        "        ?award rdfs:label ?awardLabel.\n",
        "        FILTER(LANG(?awardLabel) = \"en\")\n",
        "      }}\n",
        "\n",
        "      # Fetch educational institutions\n",
        "      OPTIONAL {{\n",
        "        ?scientist wdt:P69 ?educationInstitution.\n",
        "        ?educationInstitution rdfs:label ?educationInstitutionLabel.\n",
        "        FILTER(LANG(?educationInstitutionLabel) = \"en\")\n",
        "      }}\n",
        "\n",
        "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }}\n",
        "    GROUP BY ?scientist ?scientistLabel ?birthDate ?deathDate ?birthPlaceLabel\n",
        "    LIMIT {limit}\n",
        "    \"\"\"\n",
        "    data = execute_sparql_query(query)\n",
        "    return data['results']['bindings']\n",
        "\n",
        "# Function to fetch additional details from Wikidata API\n",
        "def get_entity_details(qid):\n",
        "    entity_dict = get_entity_dict_from_api(qid)\n",
        "    item = WikidataItem(entity_dict)\n",
        "    return {\n",
        "        'description': item.get_description('en'),\n",
        "        'aliases': item.get_aliases('en'),\n",
        "        'label': item.get_label('en')\n",
        "    }\n",
        "\n",
        "# Function to extract occupation from the description\n",
        "def extract_occupation_from_description(description):\n",
        "    description_lower = description.lower()  # Convert description to lowercase for comparison\n",
        "    for keyword in OCCUPATION_KEYWORDS:\n",
        "        if keyword in description_lower:\n",
        "            return keyword.capitalize()  # Return the matched occupation\n",
        "    return \"Unknown\"\n",
        "\n",
        "# Function to translate text to Hindi using Google Translate\n",
        "def translate_to_hindi(text):\n",
        "    if text and text != \"Unknown\":\n",
        "        try:\n",
        "            translation = translator.translate(text, dest='hi')  # Translate to Hindi\n",
        "            return translation.text\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text\n",
        "    return text\n",
        "\n",
        "def clean_date(date_string):\n",
        "    \"\"\"\n",
        "    Clean the date string to remove timestamp and keep only the date part.\n",
        "    Assumes date is in ISO format (YYYY-MM-DD)\n",
        "    \"\"\"\n",
        "    if date_string == 'Unknown':\n",
        "        return 'Unknown'\n",
        "\n",
        "    try:\n",
        "        # Split the date and take the first part (date without time)\n",
        "        cleaned_date = date_string.split('T')[0]\n",
        "\n",
        "        # Split the date into year, month, day\n",
        "        year, month, day = cleaned_date.split('-')\n",
        "\n",
        "        # Convert to Hindi numerals\n",
        "        def to_hindi_numerals(number):\n",
        "            hindi_numerals = {\n",
        "                '0': '०', '1': '१', '2': '२', '3': '३', '4': '४',\n",
        "                '5': '५', '6': '६', '7': '७', '8': '८', '9': '९'\n",
        "            }\n",
        "            return ''.join(hindi_numerals.get(digit, digit) for digit in number)\n",
        "\n",
        "        # Convert year to Hindi numerals\n",
        "        year_hindi = to_hindi_numerals(year)\n",
        "\n",
        "        # Return in a more readable Hindi format\n",
        "        return f\"{year_hindi}\"\n",
        "\n",
        "    except Exception:\n",
        "        return date_string\n",
        "\n",
        "def process_and_save_scientist_data(scientist_data):\n",
        "    # Initialize a dictionary to store scientist details\n",
        "    scientists_dict = {}\n",
        "    scientists_description_dict = {}  # New dictionary to store descriptions\n",
        "\n",
        "    for scientist in scientist_data:\n",
        "        qid = scientist['scientist']['value'].split('/')[-1]\n",
        "        label = scientist['scientistLabel']['value']\n",
        "\n",
        "        # Clean birth and death dates\n",
        "        birth_date = clean_date(scientist.get('birthDate', {}).get('value', 'Unknown'))\n",
        "        death_date = clean_date(scientist.get('deathDate', {}).get('value', 'Unknown'))\n",
        "\n",
        "        birth_place = scientist.get('birthPlaceLabel', {}).get('value', 'Unknown')\n",
        "\n",
        "        # Extract awards and educational institutions\n",
        "        awards = scientist.get('awards', {}).get('value', 'Unknown')\n",
        "        education_institutions = scientist.get('educationInstitutions', {}).get('value', 'Unknown')\n",
        "\n",
        "        # Get additional data from Wikidata API\n",
        "        extra_details = get_entity_details(qid)\n",
        "\n",
        "        # Extract occupation from the description\n",
        "        description = extra_details['description']\n",
        "        occupation_from_description = extract_occupation_from_description(description)\n",
        "\n",
        "        # Translate values to Hindi\n",
        "        name_hindi = translate_to_hindi(label)\n",
        "        birth_place_hindi = translate_to_hindi(birth_place)\n",
        "        occupation_hindi = translate_to_hindi(occupation_from_description)\n",
        "        description_hindi = translate_to_hindi(description)\n",
        "        awards_hindi = translate_to_hindi(awards)\n",
        "        education_institutions_hindi = translate_to_hindi(education_institutions)\n",
        "\n",
        "        # Translate aliases to Hindi\n",
        "        aliases_hindi = [translate_to_hindi(alias) for alias in extra_details['aliases']]\n",
        "\n",
        "        # Create a nested dictionary for each scientist\n",
        "        scientist_info = {\n",
        "            'QID': qid,\n",
        "            'Name': name_hindi,\n",
        "            'BirthDate': birth_date,\n",
        "            'DeathDate': death_date,\n",
        "            'BirthPlace': birth_place_hindi,\n",
        "            'Occupation': occupation_hindi,\n",
        "            'Description': description_hindi,\n",
        "            'Aliases': aliases_hindi,\n",
        "            'Awards': awards_hindi,\n",
        "            'EducationalInstitutions': education_institutions_hindi\n",
        "        }\n",
        "\n",
        "        scientists_dict[label] = scientist_info\n",
        "\n",
        "        # Generate and store template sentences\n",
        "        template_sentences = generate_template_sentences(scientist_info)\n",
        "        scientists_description_dict[label] = {\n",
        "            'Name': name_hindi,\n",
        "            'TemplateSentences': template_sentences\n",
        "        }\n",
        "\n",
        "    # Save the scientist data to a CSV file\n",
        "    df = pd.DataFrame.from_dict(scientists_dict, orient='index')\n",
        "    df.to_csv(\"scientists_data_hindi.csv\", index=False, encoding='utf-8')\n",
        "    print(\"Scientist data saved to scientists_data_hindi.csv\")\n",
        "\n",
        "    # Save the template sentences to another CSV file\n",
        "    df_desc = pd.DataFrame.from_dict(scientists_description_dict, orient='index')\n",
        "    df_desc.to_csv(\"scientists_description_hindi.csv\", index=False, encoding='utf-8')\n",
        "    print(\"Scientist description templates saved to scientists_description_hindi.csv\")\n",
        "\n",
        "# Function to generate template sentences for each scientist\n",
        "def generate_template_sentences(scientist_info):\n",
        "    # Template sentence generation\n",
        "    template_sentences = []\n",
        "\n",
        "    # Generate individual sentences with non-Unknown information\n",
        "    sentences_parts = []\n",
        "\n",
        "    if scientist_info['BirthDate'] != 'Unknown' and scientist_info['BirthPlace'] != 'Unknown':\n",
        "        sentences_parts.append(f\"{scientist_info['Name']} का जन्म {scientist_info['BirthDate']} में {scientist_info['BirthPlace']} में हुआ था।\")\n",
        "\n",
        "    if scientist_info['DeathDate'] != 'Unknown':\n",
        "        sentences_parts.append(f\"उनका निधन {scientist_info['DeathDate']} में हुआ।\")\n",
        "\n",
        "    if scientist_info['Occupation'] != 'Unknown':\n",
        "        sentences_parts.append(f\"{scientist_info['Name']} एक प्रसिद्ध {scientist_info['Occupation']} थे।\")\n",
        "\n",
        "    if scientist_info['Description'] != 'Unknown':\n",
        "        sentences_parts.append(f\"{scientist_info['Name']} के बारे में कहा जाता है: {scientist_info['Description']}।\")\n",
        "\n",
        "    if scientist_info['Awards'] != 'Unknown':\n",
        "        sentences_parts.append(f\"उन्हें {scientist_info['Awards']} से सम्मानित किया गया।\")\n",
        "\n",
        "    if scientist_info['EducationalInstitutions'] != 'Unknown':\n",
        "        sentences_parts.append(f\"{scientist_info['Name']} ने {scientist_info['EducationalInstitutions']} से अपनी शिक्षा प्राप्त की।\")\n",
        "\n",
        "    # Add aliases template\n",
        "    if scientist_info['Aliases'] and scientist_info['Aliases'] != ['Unknown']:\n",
        "        # Join aliases with commas\n",
        "        aliases_str = \"، \".join(scientist_info['Aliases'])\n",
        "        sentences_parts.append(f\"{scientist_info['Name']} को अन्य नामों से भी जाना जाता है, जैसे {aliases_str}।\")\n",
        "\n",
        "    # Combine sentences to make a coherent narrative\n",
        "    if sentences_parts:\n",
        "        combined_sentence = \" \".join(sentences_parts)\n",
        "        template_sentences.append(combined_sentence)\n",
        "\n",
        "    return template_sentences\n",
        "\n",
        "# Main function to run the data collection and preprocessing process\n",
        "def main():\n",
        "    print(\"Querying Wikidata for scientists...\")\n",
        "    scientist_data = get_scientist_data(limit=20)  # You can increase the limit\n",
        "    process_and_save_scientist_data(scientist_data)\n",
        "    print(\"Data collection and preprocessing complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCqSgkmAdNWh",
        "outputId": "2f31d3ba-4ec4-42b6-dcaf-8b9267c05e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying Wikidata for scientists...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqVKUaTRdWzq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}